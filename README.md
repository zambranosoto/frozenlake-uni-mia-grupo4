# Maestr√≠a en Inteligencia Artificial
## UNI 2024-2
- **Curso:** Aprendizaje por Refuerzo
- **Integrantes Grupo 4:**
  - Acevedo Mart√≠nez, Ruddy.
  - Fuertes Malca, Ayrton.
  - Sol√≠s Almerco, Ricardo.
  - Vilcahuam√°n Dolorier, Dennis.
  - Zambrano Soto, Jos√©.

# Proyecto: Resoluci√≥n del FrozenLake-v1 con Programaci√≥n Din√°mica


## Descripci√≥n del Proyecto
Este proyecto implementa una soluci√≥n al entorno **FrozenLake-v1** de la librer√≠a **Gymnasium** utilizando el m√©todo de **Programaci√≥n Din√°mica** (*Value Iteration*). El objetivo es encontrar la pol√≠tica √≥ptima que permita al agente alcanzar la meta en un lago helado sin caer en huecos.

FrozenLake es un entorno de aprendizaje por refuerzo en el que el agente se mueve sobre una cuadr√≠cula representada como un lago helado. Las acciones del agente pueden ser determin√≠sticas o estoc√°sticas, lo que a√±ade un desaf√≠o adicional al aprendizaje.

---

## Funcionalidad Principal

El programa incluye una **pantalla de selecci√≥n de entorno** que permite al usuario elegir entre una cuadr√≠cula de **4x4** o **8x8** para FrozenLake. Esta funcionalidad facilita explorar y probar diferentes configuraciones del entorno antes de iniciar el entrenamiento y la evaluaci√≥n.

---

## Objetivos
- Implementar el m√©todo de **Iteraci√≥n de Valores** (Value Iteration) para resolver el entorno FrozenLake-v1.
- Obtener la **pol√≠tica √≥ptima** y probarla en m√∫ltiples episodios.
- Generar indicadores de desempe√±o, como el **n√∫mero promedio de pasos**, la **tasa de √©xito**, y el **promedio de recompensa por episodio**.
- Visualizar los resultados a trav√©s de gr√°ficos y tablas para mostrar c√≥mo el agente converge a la pol√≠tica √≥ptima.

---

# 1. Modelado del Problema

## Definici√≥n del entorno
**FrozenLake-v1** es un entorno de aprendizaje por refuerzo donde un agente debe navegar por una cuadr√≠cula helada para alcanzar un objetivo, evitando caer en huecos. El entorno puede ser de tama√±o **4x4** o **8x8**, y est√° representado como un espacio de estados finito.
No hay un **"oponente"** expl√≠cito, pero la **estocasticidad** en las transiciones del entorno puede considerarse como un factor adverso.

### Entorno 4x4:
![Entorno 4x4](/frozenlake/img/4x4.png)

### Entorno 8x8:
![Entorno 8x8](/frozenlake/img/8x8.png)

## Modelado de estados y acciones

### - Estados:
Los estados est√°n definidos por la posici√≥n del agente en la cuadr√≠cula. En un entorno de tama√±o n√ón, hay n2 estados posibles. Cada celda puede ser:
- Inicio (S)
- Meta (G)
- Camino seguro (F)
- Hueco (H)

### - Acciones:
El agente puede realizar cuatro acciones:
- 0: Moverse hacia la izquierda
- 1: Moverse hacia abajo
- 2: Moverse hacia la derecha
- 3: Moverse hacia arriba

### - Transiciones:
El entorno es estoc√°stico; es decir, las acciones tienen una probabilidad de no ejecutarse de manera exacta, desviando al agente hacia direcciones no deseadas.

# 2. Soluci√≥n
## M√©todo utilizado
Se utiliz√≥ Programaci√≥n Din√°mica mediante el algoritmo de Iteraci√≥n de Valores (Value Iteration). Este m√©todo calcula la funci√≥n de valor
V(s) para cada estado, actualiz√°ndola iterativamente hasta la convergencia. Una vez que
V(s) converge, se extrae la pol√≠tica √≥ptima œÄ(s) que maximiza la recompensa esperada.

### - Ecuaci√≥n de Iteraci√≥n de Valores:
$$
V(s) \leftarrow \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V(s') \right]
$$
> Donde:
> - P(s‚Ä≤‚à£s,a) es la probabilidad de transici√≥n al estado s‚Ä≤ desde s al realizar la acci√≥n a.
> - R(s,a,s‚Ä≤) es la recompensa por esa transici√≥n. 
> - ùõæ es el factor de descuento.

## Implementaci√≥n y ejecuci√≥n
El algoritmo fue implementado en Python utilizando la librer√≠a **Gymnasium** para el entorno y herramientas adicionales para la visualizaci√≥n y evaluaci√≥n. La pol√≠tica √≥ptima fue evaluada en m√∫ltiples episodios para medir su desempe√±o.

# 3. Resultados
## Evaluaci√≥n
- **Tasa de √©xito:** El agente alcanz√≥ la meta en un X% de los episodios evaluados. 
- **Recompensa promedio:** Recompensa acumulada obtenida en cada episodio, entre la cantidad de episodios evaluados.
- **Pasos promedio:** Cantidad de Pasos acumulados dio el agente en cada episidio hasta de llegar a un estado terminal: Hueco(H) o Meta(G), entre la cantidad de episodios evaluados.

## Visualizaciones
- **Convergencia:** Se gener√≥ una gr√°fica que muestra c√≥mo la funci√≥n de valor converge tras cada iteraci√≥n.
![Convergencia](/results/convergence.png)


- **Mapa de calor de la pol√≠tica:** Ilustra las acciones preferidas en cada estado seg√∫n la pol√≠tica √≥ptima. Donde las acciones posibles son:
  - 0: Izquierda 
  - 1: Abajo 
  - 2: Derecha 
  - 3: Arriba

  ![Mapa de calor de la pol√≠tica](/results/policy_heatmap.png)


- **Recompensas promedio:** Presenta la evoluci√≥n de las recompensas obtenidas por el agente durante los episodios de evaluaci√≥n.
![Recompensas promedio](/results/average_rewards.png)


- **Mapa de Calor de los Valores de los Estados:** muestra el mapa de calor de los valores calculados para cada estado del entorno.
![Mapa de Calor de los Valores de los Estados](/results/value_heatmap.png)

# 4. An√°lisis
## Interpretaci√≥n de los resultados
- **Convergencia:** La funci√≥n de valor V(s) convergi√≥ r√°pidamente debido a la naturaleza peque√±a del entorno.
- **Tasa de √©xito:** Aunque la pol√≠tica es √≥ptima, la estocasticidad del entorno afecta el desempe√±o, lo que explica que en algunos episodios el agente caiga en huecos.
- **Recompensa promedio:** Refleja el balance entre episodios exitosos y no exitosos.

## Comparaciones
Se prob√≥ el entorno con diferentes tama√±os 4√ó4 y 8√ó8:
- En el tama√±o **4√ó4**, la convergencia fue m√°s r√°pida y el desempe√±o fue mejor debido a la menor complejidad del entorno.
- En el tama√±o **8√ó8**, el agente requiri√≥ m√°s iteraciones y episodios para obtener un desempe√±o consistente.

## Factores determinantes
- **Estocasticidad del entorno:** El principal desaf√≠o para el agente es la imprevisibilidad de las transiciones. 
- **Factor de descuento (ùõæ):** Valores cercanos a 1 favorecen decisiones a largo plazo, mientras que valores menores priorizan recompensas inmediatas. 
- **Tama√±o del entorno:** Entornos m√°s grandes requieren m√°s tiempo de c√≥mputo y episodios de evaluaci√≥n.

---

## Estructura del Proyecto

```plaintext
‚îú‚îÄ‚îÄ frozenlake/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Inicializa el paquete.
‚îÇ   ‚îú‚îÄ‚îÄ environment.py      # Configuraci√≥n y funciones relacionadas con el entorno.
‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py       # Implementaciones de programaci√≥n din√°mica (value iteration).
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Funciones de utilidad (guardar/cargar pol√≠tica, visualizaciones).
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py       # C√≥digo para probar pol√≠ticas y generar indicadores.
‚îú‚îÄ‚îÄ main.py                 # Punto de entrada principal para ejecutar el programa.
‚îú‚îÄ‚îÄ results/                # Carpeta donde se almacenan los gr√°ficos y resultados.
‚îî‚îÄ‚îÄ README.md               # Documentaci√≥n del proyecto. 
```

---

# 5. Instalaci√≥n

## **Clonar el repositorio**:
   ```bash
   git clone https://github.com/zambranosoto/frozenlake-uni-mia-grupo4.git
   cd frozenlake-uni-mia-grupo4
   ```

## **Dependencia de librer√≠as**:
  ```plaintext
  gymnasium==0.27.1       # Librer√≠a para crear y manejar entornos de aprendizaje por refuerzo
  pygame==2.1.3           # reemplaza con la versi√≥n que prefieras
  matplotlib==3.7.1       # Para graficar mapas de calor y curvas de convergencia
  numpy==1.25.0           # Operaciones num√©ricas
  tk==0.1.0               # Interfaz gr√°fica (tkinter)
  ```
